% -*- mode: LaTex; outline-regexp: "\\\\section\\|\\\\subsection";fill-column: 80; -*-
\documentclass[12pt]{article}
\usepackage[longnamesfirst]{natbib}
\usepackage[usenames]{color}
\usepackage{graphicx}  % Macintosh pdf files for figures
\usepackage{bbm}       % one symbol
\usepackage{amssymb}   % Real number symbol {\Bbb R}
\input{../../standard}

% --- margins
\usepackage{../../sty/simplemargins}
\setleftmargin{1in}   % 1 inch is NSF legal minimum
\setrightmargin{1in}  % 1 inch is NSF legal minimum
\settopmargin{1in}    % 1 inch is NSF legal minimum
\setbottommargin{1in} % 1 inch is NSF legal minimum

% --- Paragraph split, indents
\setlength{\parskip}{0.00in}
\setlength{\parindent}{0in}

% --- Line spacing
\renewcommand{\baselinestretch}{1.3}

% --- Margins
\setlength{\topmargin}{-0.5in}
\setlength{\oddsidemargin}{-0.1in}
\setlength{\textheight}{9.0in}
\setlength{\textwidth}{6.5in}

% --- page numbers
\pagestyle{empty}  % so no page numbers

% --- Hypthenation
\sloppy  % fewer hyphenated
\hyphenation{stan-dard}
\hyphenation{among}

% --- Customized commands, abbreviations
\newcommand{\TIT}{{\it  {\tiny Risk of sequential tests (\today)}}}

% --- Header
\pagestyle{myheadings}
\markright{\TIT}

% --- Title

\title{ Risk of Sequential Testing with Alpha Investing }
\author{
        Dean P. Foster and Robert A. Stine\thanks{Research supported by NSF grant DMS-1106743 }  \\
        Department of Statistics            \\
        The Wharton School of the University of Pennsylvania \\
        Philadelphia, PA 19104-6340                          \\
        www-stat.wharton.upenn.edu/$\sim$stine 
}

\date{\today}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle 
%------------------------------------------------------------------------

\abstract{ 

 Streaming feature selection evaluates potential explanatory variables
 sequentially rather than all at once.  This approach produces novel challenges
 for variable selection.  Spending rules such as alpha investing control the
 rate of false discoveries, but little is known about the risk of the resulting
 estimator.  We provide here a computational framework for finding and comparing
 the finite-sample, worst-case risk of streaming selection.  Our findings
 demonstrate that in all but very sparse models, estimators allowed to have
 larger rates of false discoveries produce smaller risk.  

}

%------------------------------------------------------------------------
\vspace{0.05in}

\noindent
{\it Key Phrases: Bellman equations, hard thresholding, streaming feature
 selection, testimator, variable selection}

\clearpage


% ----------------------------------------------------------------------
\section{ Introduction }
% ----------------------------------------------------------------------


 Streaming feature selection constructs a predictive model by choosing
 explanatory variables from a sequence offered by an exogenous source.  Rather
 than evaluate these variables simultaneously, streaming selection evalutes them
 one-at-a-time.  Greedy searches like stepwise regression consider the full
 batch of, say, $p$ potential explanatory variables together, choosing at the
 first step the predictor $X_{(1)}$ that obtains the best fitting model.  In
 contrast, streaming selection evaluates the offered predictors sequentially as
 $X_1, \, X_2, \ldots$, performing the evalation of $X_j$ in the context of the
 model produced by picking from $X_1, \ldots, X_{j-1}$.  Hence, streaming
 selection does not require the full set of explanatory variables at the start
 of the search and is free to use the results of evaluating initial variables to
 guide the search for those to add subsequently.  For example, if it adds $X_j$
 to the model, the streaming search might then be expand add interactions $X_j
 \, X_k$ to the queue of possible variables to consider.  Streaming selection
 can thus rapidly explore collections of explanatory variables that are larger
 than typically considered with conventional methods; the slowest step in
 forward stepwise regression is the calculation of the $X'X$ matrix.
  \citet{fosterlin10} show examples selecting from up to $p$=100,000 explanatory
 variables.


 Streaming selection poses a challenge, however, for variable selection.
  Although it is advantageous to avoid simultaneously evaluating every
 predictor, the absence of a fixed set of features in streaming selection
 requires a different type of selection criterion from those commonly used.  For
 example, suppose the search begins with a list of $p$ possible features $X_1,
 X_2, \ldots, X_p$.  As mentioned previously, the search could expand to include
 interactions in $X_j$ once $X_j$ joins the model.  If the search is limited to
 second-order interactions (one could allow higher order interactions as well),
 then the maximum number of possible explanatory variables is $m = p(p+1)/2$
 variables.  Since few of these would be considered, it would be very
 conservative to combine $m$ with a criterion such as AIC, BIC, or RIC.
  Similarly, selection using FDR requires the full set of marginal $p$-values.


 Alpha investing \citep{fosterstine08} is a sequential testing procedure
 designed to support streaming feature selection.  Because alpha investing can
 test an infinite sequence of hypotheses, it is well-matched to a search of an
 unbounded collection of features that is too large to manipulate
 simultaneously.  Rather than test multiple hypotheses at once, alpha investing
 tests hypotheses one-at-a-time in a specified order.  Alpha investing begins
 with an initial allowance for Type I error that is called its alpha wealth.
  Each test consumes some of the available alpha wealth, as in the alpha
 spending rules used in clinical trials.  Alpha investing differs from these and
 overcomes the conservatism of alpha spending rules, which include the
 Bonferroni method, by earning a contribution to the alpha wealth available for
 subsequent tests for each rejected null hypothesis.  Thus rejections beget more
 rejections.  Alpha investing further allows one to test an infinite stream of
 hypotheses, accommodate dependent tests, and incorporate domain knowledge.
 

 Though flexible, alpha investing controls the expected number of false
 rejections.  Controlling the false discovery rate with alpha investing guards
 against overfitting in variable selection.  With alpha inveseting, one can
 guarantee that on average not more than, say, 5\% of the rejected hypotheses
 spuriously add a predictor to the model.  When building a predictive model,
 however, controlling the false discovery rate is frequently secondary to
 obtaining a more predictive model.  Control of the false discovery rate does
 not imply that one will find the most predictive model possible.  It only
 guarantees that a high percentage of chosen features are in fact useful.  The
 risk of the implied estimator is more relevant.


 Our analysis here considers the cumulative risk of a sequence of testimators
 implied by testing a sequence of null hypotheses.  A testimator is also known
 as a keep-or-kill estimator or a hard thresholding estimator.  The estimator of
 a parameter $\mu$ is zero unless a test of the null hypothesis that claims $\mu
 = 0$ is rejected.  The estimation problem we consider is a simplified version
 of the variable selection problem that avoids issues related to the
 collinearities among the explanatory variables.  Rather than observe a sequence
 of slope estimates, we assume that the observed data are a finite sequence of
 $p$ random variables $Y_j \sim N(\mu_j,1)$.  We obtain a testimator from a
 sequence of two-sided tests.  For coordinate $j$, the level of the test is
 $\al_j$.  The test rejects $H_j: \mu_j = 0$ if $z_{\alpha_j}^2 \le Y_j^2$,
 where $z_{\al_j}$ denotes the corresponding two-sided critical value $z_{\al_j}
 = \Phi^{-1}(1-\al_j/2)$.  The resulting estimator has $j$th element
 \begin{equation}
    \hat\mu_j = \left\{
      \begin{array}{cc}
            Y_j  & z_{\al_j}^2 \le Y_j^2 \cr
            0  & \mbox{ otherwise. }
      \end{array} \right.
 \label{eq:muhat}
 \end{equation}
 We collect these estimates in the vector $\hat\mu = (\hat\mu_1, \ldots,
 \hat\mu_p)'$ and similarly write $\mu = (\mu_1, \ldots, \mu_p)'$ and $Y = (Y_1,
 \ldots, Y_p)'$.  The risk of $\hat\mu$ is
 \begin{equation}
    R(\hat\mu, \mu) 
      = \ev_\mu \normsq{\hat\mu - \mu} \;,
 \label{eq:risk}
 \end{equation}
 where $\normsq{x} = x'x$ for vectors $x$.


 Minimax bounds for the risk $R(\hat\mu,\mu)$ are well-understood in the
 classical context of model selection that simultaneously fixes the level $\al_j
 = \al$ of all of the tests.  Let $\hat\mu(\al)$ denote the testimator with
 fixed level $\al$ for all $p$ tests.  \citet{fostergeorge94} define a ratio
 known as the risk inflation of $\hat\mu(\al)$. \citep[][obtain similar
 results.]{donohojohnstone94} The risk inflation of $\hat\mu(\al)$ is the
 supremum of the ratio of the risk of the testimator with level $\al$ the risk
 of a testimator that consults an oracle for the best possible level.  Their
 results imply that the risk inflation of $\hat\mu(\al)$ is asymptotically $2
 \log p$,
 \begin{equation}
    2 \log p - o(\log p) 
    \le
    \sup_\mu  \frac{1 + R(\hat\mu(\al), \mu)}
                   {1 + \inf_\eta{R(\hat\mu(\eta), \mu)}}  
    \le 
    2 \log p + 1 \;.
 \label{eq:ri}
 \end{equation}
 Foster and George also show that the testimator $\hat\mu(1/p)$ -- essentially
 the Bonferoni estimator -- obtains the risk inflation threshold.  The constant
 1 in the ratio arises in the setting of regression models in which one always
 estimates the intercept.  As a practical device, the constant also avoids
 dividing by zero if every $\mu_j = zero$.


 Though suggestive, these results do not apply directly to the estimator derived
 from alpha investing because they describe multivariate estimates produced
 simultaneously rather than sequentially.  This change in the timing of
 information affects both the estimator and the competing oracle.  As described
 in the following section, sequential testing with alpha investing allows the
 $\al$-level to vary over the sequence of tests.  In general, the level of the
 $j$th test, $\al_j$, is free to depend on whether prior tests reject $H_k$, $k
 < j$.  Classical risk inflation has a fixed rule for selecting coordinates that
 is set prior to observing data.  The dynamic aspect of sequential testing also
 impacts the choice of a relevant oracle.  In \eqn{eq:ri}, the oracle picks a
 fixed level that minimizes its risk.  For sequential testing, the oracle too is
 free to vary its level depending on the behavior of the competing data-driven
 estimator.


 \ras{ Revise roadmap paragraph }
 The following section describes the use of alpha-investing for setting the
 levels $\al_j$ of the tests that determine the testimator $\hat\mu$.  Section 3
 describes the computations that solve a set of Bellman equations.  Our
 methodology bounds the risk of any sequential testimator.  In the spirit of
 risk inflation and oracle bounds, we compute the convex set of attainable
 risks:
 \begin{equation}
   \CC(\hat\gamma) 
      = \{(x,y): \exists \mu \mbox{ for which }
                 x=R(\tilde\mu,\mu), \, y = R(\hat\mu_{\hat\gamma},\mu)\} \;,
 \label{eq:C}
 \end{equation}
 where $\tilde\mu$ identifies an oracle estimator of $\mu$ that is defined in
 Section 3.  The boundary of $\CC$ produces exact results that are comparable to
 those obtained in asymptotic multivariate setting.  Section 4 displays the risk
 of these procedures, and we conclude with a summary and discussion of open
 issues in Section 5.


% ---------------------------------------------------------------------------
\section{ Alpha-investing }
% ---------------------------------------------------------------------------

 An alpha-investing rule \citep{fosterstine08} determines the level for testing
 $H_j$ based on its alpha wealth, a limit on the maximum level that depends on
 the results of prior tests.  The procedure is most easily described by
 explaining the first few steps.  The process begins with an initial allocation
 $W_0$ \marginpar{$W_0$} of alpha wealth.  The choice of this initial alpha
 wealth is an important aspect of our analysis of the risk of $\hat\mu$.  An
 alpha-investing rule can test $H_1$ at any level up to the total available
 alpha wealth, $0 \le \alpha_1 \le W_0$.  The level $\al_1$ is `invested' and
 cannot be used for subsequent tests.  We say that $\al_1$ is invested rather
 than spent because of the way in which the outcome of the test of $H_1$
 influences the subsequent wealth.  Let $p_1$ denote the p-value of the test of
 $H_1$.  If $p_1 \le \alpha_1$, the test rejects $H_1$.  In this case, the
 alpha-investing rule earns a contribution $\omega > 0$ \marginpar{$\omega$} to
 its alpha wealth; otherwise, the alpha wealth available to test $H_1$ falls to
 $W_1 = W_0 - \alpha_1$.  In general, the alpha wealth available for the test of
 $H_{j+1}$ is \marginpar{$W_j$}
 \begin{equation}
    W_{j} = W_{j-1} - \alpha_j + \omega \, I_{\{p_j < \al_j\}}
 \label{eq:Wj}
 \end{equation}
 Alpha investing thus resembles alpha spending used in clinical trials, with the
 key distinction that rejecting a hypothesis earns an additional allocation
 $\omega$ of alpha-wealth for subsequent testing.  \ras{reference to QPD and
 enhanced alpha investing}


 Because rejecting a null hypothesis makes it easier to reject other null
 hypotheses, it is important that alpha investing controls the rate of false
 rejections.  To this end, \citet{fosterstine08} show that alpha investing
 controls a sequential version of mFDR.  Let $D(j)$ count the number of
 hypothesis rejected in the first $j$ tests, and let $V(j) \le D(j)$ denote the
 number of {\em false} rejections through the first $j$ tests.  The sequential mFDR is
 \begin{equation}
    \mbox{mFDR}_\eta(j) = \frac{\ev V(j)}{\eta + \ev R(j)} \;, \eta > 0.
 \label{eq:mFDR}
 \end{equation}
 The similar false discovery rate is essentiallyt the expected value of the
 ratio rather than the ratio of expected values.  The constant $\eta$ in the
 denominator of mFDR avoids dividing by zero under the complete null hypothesis
 in which all $\mu_j = 0$.  If $W_0 \le \omega$, then alpha-investing rules
 control $\mbox{mFDR}_\eta(p) \le \omega$, and this result implies weak control
 of the family wide error rate.  Furthermore, the index $j$ in \eqn{eq:mFDR} is
 allowed to be an arbitrary stopping time, such as the occurrence of the $k$th
 rejection.

 
 Alpha-investing rules are quite general.  The underlying theory requires only
 that the level of the test of $H_j$ is bounded by the available wealth
 $W_{j-1}$ and that the test indeed have level $\al_j$, conditional on the
 outcomes of prior tests.  Otherwise, an alpha-investing rule is allowed to use
 the pattern of prior rejections as captured by the sequence of wealths.  We can
 capture this dependence by thinking of the investing rule ${\cal A}$ as a
 function of the sequence of prior wealths $W_0^j = \{W_0, W_1, \ldots, W_j\}$.
  A rule is then a map from the sequence of wealths to a number on the interval
 0 to the current wealth:
 \begin{equation}
    {\cal A}: W_0^j \mapsto [0, \min(W_j,1)]    
 \label{eq:A}
 \end{equation}
 For example, the rule $\cal A$ can set the wealth of the next test higher or
 lower depending upon how long since the last rejection or the number of
 rejections to this point.  Though alpha investing allows this complete
 generality, we focus our attention on a simpler class of investing rules that
 depend only on the current wealth, ${\cal A}(W_0^j) = {\cal A}(W_j)$.  The
 simplest representative of this class of alpha-investing rules are geometric
 rules that invest a fixed percentage of the available wealth on each test:
 \begin{equation}
    {\cal A}_g(W_0^j, \psi) = \psi W_j, \quad  0 < \psi \le 1.
 \label{eq:Ageo}
 \end{equation}
 For example, the geometric rule with $\psi = 0.25$ invests one-fourth of the
 its alpha wealth $W_j$ in the test of $H_{j+1}$.  Generally speaking, since the
 alpha wealth increases after a rejection, the rule ${\cal A}_g$ spends more
 following a rejection and then gradually -- depending on $\psi$ -- reduces the
 level of subsequent tests.
 

 To motivate other investing rules, we offer a different motivation for the
 geometric rule \eqn{eq:Ageo}.  The geometric rule describes how one spends down
 the available wealth until the next rejection.  If the initial wealth is $W_0$,
 then the alpha wealth invested in the $j$th test (assuming no intervening test
 rejects) is
 \begin{equation}
    \al_j = W_0\; \psi \; (1-\psi)^{j-1} \;, \quad j = 1, 2, \ldots\;.   
 \label{eq:alj1}
 \end{equation}
 It will be useful in our computations to define an investing rule for any value
 $0 < x$ rather than integers that represent test indices.  In particular, we
 want the bid to be only a function of the available wealth and not require
 other information, such as an index counting the tests since the last
 rejection.  We illustrate this representation for the geometric and then
 provide other examples.  


 To this end, consider the following alternative description of geometric
 investing.  Rather than define the investing rule using a discrete distribution
 on $j=1,\,2,\ldots$ as in \eqn{eq:alj1}, use a continuous density.  In place of
 the discrete distribution, define the density function on $[1,\infty]$:
 \begin{equation}
   \al_{g}(x,\psi) = \smfrac{1}{c_g} \; \psi\,(1-\psi)^{x-1}, \quad 1 \le x\;,
 \label{eq:alg}
 \end{equation}
 where the normalizing constant $c_g$ is
 \begin{displaymath}
   \quad c_g = \int_1^\infty \psi(1-\psi)^{x-1} = -\psi/\log (1-\psi)\;.     
 \end{displaymath}
 The wealth invested in the $j$th subsequent test is defined by the integral of
 $\al_g(x)$ from $x=j$ to $x=j+1$,
 \begin{equation}
    \al_j = W_0 \int_j^{j+1} \al_g(x,\psi) dx = W_0 \, \psi\; (1-\psi)^{j-1} \;,
 \label{eq:alj2}
 \end{equation}
 matching the investment given by the discrete rule \eqn{eq:alj1}.  Now define
 the tail integral that shows the amount of wealth that remains,
 \begin{equation}
    W_g(x) = W_0 \int_x^\infty \al_g(t,\psi) dt = W_0 (1-\psi)^{x-1}\;.
 \label{eq:Wg}
 \end{equation}
 For integers $j$, $W_g(j)$ is the wealth remaining after $j$ tests that fail to
 reject.  By inverting this tail integral, we obtain the alpha wealth to invest
 from the available wealth,
 \begin{equation}
   {\cal A}_g(w,\psi) = \al_g(W_g^{-1}(w),\psi) = \psi\,w \;,
 \label{eq:Ag}
 \end{equation}
 as in the description of geometric investing given in \eqn{eq:Ageo}.  This
 construction uses the inverse of the tail wealth to determine a `test index'
 $W_g^{-1}(w)$ that corresponds to the current input wealth $w$.  The rule then
 invests as if bidding down from the initial wealth, but no longer tracks a
 discrete index.  Note that this rule works for any wealth $0 < W < \infty$; the
`index' $W_g^{-1}(w)$ might be negative, but that is okay because the function
$\al_g$ is monotone decreasing from $\infty$ to  \ras{how to say this clearly}.


 Geometric rules invest a fixed share of the available wealth to each test.  The
 following rule varies the invested share, investing a smaller share as the
 wealth drops.  The form of the density used in the construction of this rule is
 similar to the universal prior for integers defined by \citet{rissanen83}, and
 hence we call this a universal investing rule and identify the related terms by
 a subscript $u$.  The density representing this rule is
 \begin{equation}
   \al_u(x) = \frac{\log 2}{(x+1) \, (\log (x+1))^2}, \quad 1 \le x,
 \label{eq:alu}
 \end{equation}
 where it is easily checked that $\int_1^\infty \al_u(x) dx = 1$.  The
 corresponding investing rule is
 \begin{equation}
   {\cal A}_u(w) = \al_u(W_u^{-1}(w))  
 \label{eq:Auniv}
 \end{equation} 
 with the tail wealth $W_u(x) = W_0 \int_x^\infty \al_u(t) dt = W_0\, (\log
 2)/\log(x+1)$ so that $W^{-1}(w) = 2^{W_0/w}-1$.



Rather than invest at a
 fixed rate, a universal rule invests a higher percentage of the available
 wealth in the test of $H_{j+1}$ when the wealth $W_j$ is large than when the
 available wealth is small.  The universal rule is based upon the discrete
 distribution with probabilities
 \begin{equation}
   u(j) = \frac{c}{ j (\log (j+1))^2}, \qquad 
                 j =  1,\,2,\,\ldots \;.
 \label{eq:univ}
 \end{equation}
 The consant $c \approx 3.388$ normalizes the sum so that $\sum_j u(j) = 1$.
  More elaborate forms of the universal distribution use the so-called log-star
 function, defined as $\log^* x = \log x + \log \log x + \cdots$, where the sum
 accumulates only positive terms.  (For example, $\log^{*} 8 = \log 8 + \log
 \log 8$.)  The disribution in \eqn{eq:univ} simply uses the first two summands
 of the $\log^{*}$ function.
 
 
 To obtain an alpha-investing rule that is dependent only upon the current
 wealth requires a few adjustments.  First, assume that the maximum alpha wealth
 attainable by the procedure is bounded by $M$.  (This constraint can be
 enforced by requiring ${\cal A}(M) = \omega$.)  Next, define the tail sum
 \begin{equation}
   \ol{U}(k) = \sum_{j=k}^{\infty} u(j) \;.
 \label{eq:Ubar}
 \end{equation}
 $\ol{U}(k)$ is proportional to the alpha wealth remaining if the first $k-1$
 tests fail to reject any hypothesis.  The alpha level of the test of a
 hypothesis can then be defined as the following function of the wealth,
 \begin{equation}
   {\cal A}_u(W_0^j) = {\cal A}_u(W_j) 
                     = M \; u \left(\ol{U}^{-1}(W_j/M) \right) \;.
 \label{eq:Auniv}
 \end{equation}
 If there does not exist a $k$ such that $U(k)=W_j/M$ (that is,
 $\ol{U}^{-1}(W_j/M)$) is not an integer) then we will use an interpolation
 scheme.  We defer those details until we discuss details of the calculations in
 Section XX.


%--------------------------------------------------------------------------
\section{ Risk Analysis}
%--------------------------------------------------------------------------

We start by considering the risk of testimators in the more familiar case of a
single test or simultaneous tests.  This context allows us to introduce the
approach that we take when studying the risk of alpha investing procedures in
the sequential context.



%--------------------------------------------------------------------------
\subsection{ Scalar Risk }
%--------------------------------------------------------------------------

We first consider the squared error risk of testimators for a single parameter.
  Denote the scalar testimator based on the test with level $\al$ as
 $\hat\mu_\al(Y)$ where $Y \sim N(\mu,1)$.

The risk of $\hat\mu_\al$ is
 \begin{eqnarray}
   R(\hat\mu_\al,\mu) 
     &=& \ev(\hat\mu_\al(Y) - \mu)^2  \cr
     &=& \mu^2 \pr(Y^2 \le z_\al) 
         + \int_{y^2>z_\al} (y-\mu)^2 \phi(y) dy \;.
 \label{eq:risk_mu_al}
 \end{eqnarray}
 The first summand is the squared bias that arises if the test does not reject
 when $\mu \ne 0$.  The second is the variance of the estimator.  Figure
 \ref{fig:risk}(a) shows a graph of the risk of two testimators with $\al=0.05$
 and $\al = 0.20$.  Figure \ref{fig:risk}(b) shows the decomposition of the risk
 of $\hat\mu_{0.05}$ into bias and variance terms.  Because the variance
 component of the risk is bounded by one, the bias dominates the risk unless
 $\al$ is unusually large.


 \begin{figure}
 \caption{ \label{fig:risk} Risk of testimators. (a) Risk of testimators with
$\al$ = 0.05, 0.20 versus $\mu$. (b) Components of the risk of $\hat\mu_{0.05}$. }
 \centerline{ 
   \includegraphics[width=2.5in]{figures/risk_a}
   \includegraphics[width=2.5in]{figures/risk_b} }
 \end{figure}

 


% RAS

%--------------------------------------------------------------------------
\section{ Computation }
%--------------------------------------------------------------------------

 
 Convexity. A simple randomization argument shows that any linear combination of
 the mean points that we found is attainable.  That's not the same, however, as
 showing that the actual set is convex.  Each calculation we do (for some gamma
 or direction angle) finds the maximum value of the function in that direction.
  Hence, you cannot get further out in that direction. That removes half of the
 space; a collection of these leaves a convex interior.  That convex interior
 holds the collection of solutions.  The smaller the angle between direction
 vectors, the closer we approximate the feasible set.  Rather than get the
 intersection of {\em all} half-spaces that hold the feasible region, we git a
 finite number of them.




 We identify the boundary of the performance envelope by solving a collection of
 one-dimensional optimizations.  Figure \eqn{fi:tangent} illustrates the method
 used to identify a boundary point of PE$(f,g; r)$ that lies above the diagonal.
  Pick some value $\gamma > 0$ \marginpar{$\gamma$}; $\gamma = 1$ in the figure.
  The intercept $C^\gamma$ of the tangent line identifies the boundary value of
 the performance envelope at the point of tangency:
 \begin{equation}
     C^\gamma = \max_{\mu \in \RT} U_r(\mu,g) - \gamma \, U_r(\mu,f) 
 \label{eq:opt}
 \end{equation}
 The solution is obtained recursively as in a Bellman equation.  To express the
 recursion, expand the notation and let $C^\gamma = C_1^\gamma(A_1,0;B_1,0)$
 where $A_1=B_1=W_1$ denote the initial alpha wealths associated with the alpha
 investing rules defined by $f$ and $g$. The zeros indicate that no tests have
 occurred since the last rejection so that $f(0)$ and $g(0)$ determine the amount
 to invest in the test of $H_1$.


 \begin{figure}
 \caption{ \label{fi:tangent} The intercept of the tangent line with slope
 $\gamma = 1$ identifies a point on the boundary of the performance envelope. }
 \centerline{ \includegraphics[width=4in]{tangent} }
 \end{figure}

 
 Now consider the general case of the test of $H_j$.  Assume that the alpha
 wealth available to the two investing rules is $A_j$ and $B_j$, respectively,
 at this stage, and that it has been $\ell \le j$ tests since the last rejection
 by the first rule and $m \le j$ tests since the last rejection by the second.
  Assume also for ease of presentation that the level $\al_j = A_j f(\ell)$
 invested in the test of $H_j$ by the first investing rule is less than the
 level $\beta_j = B_j g(m)$ invested by the second ($\al_j < \beta_j$). It
 follows that, when utility is measured by the number of rejections, that we
only need a one-dimensional optimization at each test,
 \begin{eqnarray}
   C^\gamma_j(A_j,\ell;B_j,m) 
    &=& \max_{\mu \in \R } \left[ r^{}_{\mu}(\al_j) - \gamma \, r_{\mu}(\beta_j)\right. \cr
    && \;+ \quad r_{\mu}(\al_j) \qquad \quad
              C^\gamma_{j+1}(A_j+\omega-\alpha_j,0;\,B_j+\omega-\beta_j,0)  \cr
    && \;+ (r_\mu(\beta_j)-r_\mu(\al_j)) \; 
              C_{j+1}^\gamma(A_j-\alpha_j,\ell+1;\,B_j+\omega-\beta_j,0) \cr
    && \;+ \left.  (1-r_\mu(\beta_j)) \; 
              C_{j+1}^\gamma(A_j-\alpha_j,\ell+1;\,B_j-\beta_j,m+1) \right] \;,
 \label{eq:util}
 \end{eqnarray}
 with the boundary condition $C_{T+1}^\gamma = 0$.  The successive lines
 identify the expected differential in the number of rejections produced by the
 test of $H_j$, and following summands denote the subsequent expected values if
 both reject, if only the rule with the larger alpha level rejects, and if
 neither rejects.  


 Practical solution of the recursion for $C_1^\gamma$ requires a discrete
 approximation.  Notice in \eqn{eq:util} that the state of the recursion depends
 on the wealths of the two investing rules. Feasible calculation requires that
 we restrict the possible wealths to a discrete grid.  If the wealths are
 allowed to vary over any $W \ge 0$, then solving this recursion for any sizable
 $T$ is intractable.  Our approach discretizes the wealth functions so that the
 optimization occurs over a grid for each test $j$ rather than the positive
 quadrant of $\R^2$.  For each investing rule, we initialize a grid of $T+M+1$
 wealth values $w_j$, indexed from $j=M, M-1, \ldots, 1, 0, -1, \ldots, -T+1,
 -T$.  This grid holds the state of the wealth at each test, and the differences
 in adjacent wealths determine the amounts used to test the next hypothesis.
  For the rule defined by the distribution $f \in {\cal F}$, we set $w_0 = W_1$,
 $w_{-1} = w_0(1-f(0))$, $w_{-2} = w_{-1}(1-f(1)), \ldots$.  If the investing
 rule does not reject any hypotheses, these wealths are exact.  If the rule does
 reject, we accumulate the utility as though performing a randomized test that
 tosses a biased coin to decide which of the nearby wealths to spend.  Suppose
 that the alpha wealth when rejecting is $X = w_j + \omega$.  It is unlikely
 that $X$ lies at one of the grid of wealth values, so assume that $x = c \, w_k
 + (1-c) w_{k+1}$ for some $0 < c < 1$.  In this case, we treat the next test as
 a randomized test.  The test earns the expected utility from wealth $w_k$ with
 probability $(1-c)$ and from wealth $w_{k+1}$ with probability $c$. Basically,
 this approximation adds a second expectation to the sum in \eqn{eq:util}. We
 set $w_j$ for $j > 0$ somewhat arbitrarily in a manner that prevents the
 accumulation of excess wealth.  In our examples, $M=5$ with $w_i = W_1 + i
 \,\omega/3$, $i=1,2,3$, and $w_i = w_{i-1} + \omega$ for $i=4,\,5$.  Should the
 wealth reach $w_4$, then the bid for the next test is $\omega$, the amount
 earned by a rejection.  Hence, the testing does not increment the wealth beyond
 this boundary.
 

 We obtain a performance envelope by varying the competitive factor $\gamma$.
  To find the boundary points below the diagonal, we reverse the roles of the
 alpha investing rules and repeat the optimization.  As the optimization
 proceeds, we accumulate the component utilities that identify the boundary
 point.


%--------------------------------------------------------------------------
\section{ Examples }
%--------------------------------------------------------------------------
 
 The examples in this section compute the feasible risks produced by the alpha
 investing rules just described.  We also consider several choice for the payoff
 $\omega$ that controls mFDR.  The choices span from the conventional Type I
 error rate with $\omega=0.05$ to $\omega = 0.5$ and larger.  Setting
 $\omega=0.5$ implies that we allow up to half of the rejected hypotheses to be
 Type I errors.  Such a large error rate would not be used in testing, but is
 natural when trying to minimize worst case risk.  

 \clearpage

 A simple approximation suggests that alpha investing procedures should allow
 larger values for mFDR than typical in testing.  Consider the risk produced by
 a testimator defined by a test of simple hypotheses for a mean.  Suppose that a
 test of the hypotheses $H_0: \mu=0$ versus $H_a: \mu=\eta$ has level $\alpha$.
  The test rejects $H_0$ if $Y^2 > z_{\al/2}^2$ for $Y \sim
 N(\mu,1)$.  Our approximations decompose the risk of the testimator
 $\hat\mu_\al = Y\,\one{Y^2>z_{\al/2}^2}$ as shown in the following table. Note that we treat $\mu$ as a
 r.v. with probability $\pi$ on $\eta$ and 0 otherwise.

\begin{center}
\begin{tabular}{c|cc}
            &   $\hat\mu=0$            & $\hat\mu\ne 0$               \cr \hline
 $\mu=0$    &  0                       & $\al (1-\pi) (2 \log 1/\al)$ \cr
 $\mu=\eta$ &  $(\pi/2)(2 \log 1/\al)$ & $\pi/2 \cdot 1$              \cr
\end{tabular}
\end{center}

 \noindent
 These approximations use $z_\al \approx \sqrt{2 \log 1/\al}$ and estimate the
 risk at this threshold by $2 \log \al$.  The components in this table are
 summands in the decomposition
 \begin{equation}
   \ev (\hat\mu_\al - \mu)^2 
     = \sum_{C} \ev \left((\hat\mu_\al-\mu)^2 | C\right) \pr(C),
 \label{eq:decomp}
 \end{equation}
 where the conditions are $\{\mu=0, Y^2 < z_{\al/2}^2\}, \{\mu=0, Y^2 \ge
 z_{\al/2}^2\},\{\mu=\eta, Y^2 < z_{\al/2}^2\},\,\mbox{ and
 },\{\mu=\eta, Y^2 \ge z_{\al/2}^2\}$.


 Now consider the mFDR for this table:
 \begin{equation}
     mFDR = \frac{\al(1-\pi)}
                 {\al(1-\pi) + \pi/2} \;.
 \label{eq:mfdrtable}
 \end{equation}
 To perform well, the testimator should balance the risk associated with the two
 types of errors, so that \ras{Should this balance refer to {\bf both} of terms
 from the second column or just to the Type I and Type II errors? It would not
 make much difference here, but it does in the more accurate calculations. }
 \begin{equation}
   \al(1-\pi) 2 \log (1/\al) \approx (\pi/2) (2 \log(1/\al) \Rightarrow   
         \pi/2 \approx \al(1-\pi) \;.
 \label{eq:balance}
 \end{equation}
 Plugging this back in \eqn{eq:mfdrtable} gives mFDR$=\half$.

 \clearpage

 To make the problem hard, suppose that the mean
 under the alternative has been chosen to produce maximum risk for the
 testimator,
 \begin{equation}
    \mu_\al = \arg \max_\mu R(\hat\mu,\mu) \approx \mbox{[add this]}
 \label{eq:Malpha}
 \end{equation}
 Figure \ref{fi:risk} shows a plot of the risk with $\alpha=0.05$ and
 $\alpha=0.0005$, a more typical value in, say, model selection.
  \citet{fostergeorge94} show that as $\alpha \rightarrow 0$, $\mu_\al = \sqrt{2
 \log 1/\al} + o(something) \approx z_{\al/2}.$ Assume, as in a Bayesian model,
 $H_a$ holds with small probability $\pi$; in other words, we are in the 'nearly
 black' setting in which most parameters are zero.  The major contributions to
 the risk in this setting come from Type I and Type II errors.  The risk from a
 Type I error is large because the test rejects $H_0$ only if $Y$ is far from
 zero: $\alpha (1-\pi) 2 \log(1/\al).$ The risk from a Type II error is also
 large because of the size of $\mu_\al$; this risk is approximately $(\pi/2) 2
 \log(1/\al)$. (The $\pi/2$ comes from the distribution of $Y$ under $H_a$ being
 located on the rejection boundary.)  Setting mFDR equal to 1/2 balances these
two risks.  In this context,
 \begin{equation}
    mFDR = \frac{\al(1-\pi)}
                {\al(1-\pi) + \pi/2} \;.   
 \label{eq:mfdrexample}
 \end{equation}
 Balancing the risks of Type I and Type II errors implies that we set
$\pi/2=\al(1-\pi)$ and hence that we want $mFDR = 1/2$.


\clearpage

 To evaluate an alpha investing rule, we consider its performance in the
 following context.  Consider testing a sequence of $T$ null
 hypotheses $H_j: \mu_j \le 0, \, j=1,\,2,\, \ldots, \mu_T$ versus the alternatives
 $H_{j,a}: \mu_j > 0$.  \marginpar{$\mu_{1:T}$} Denote the collection of mean
 parameters $\mu_{1:T} = \{\mu_1, \mu_2, \ldots, \mu_T\}$.  The test statistics
 are $Z_j \sim N(\mu_j,1)$.  The $Z_j$ are independent and observed one at a
 time; the test of $H_k$ is made in the knowledge of prior $Z_j$ for $j<k$, but
 future $Z_j, j > k$ are unknown.  The number of tests $T$ is fixed and known at
 the start of testing.  For the purpose of calculating risks, a sequence of
 tests defines a sequence of `testimators' by setting $\hat\mu_j = Z_j$ if
 $p_j<\al_j$ and zero otherwise.
 

 

 Within this context, how does the choice of an investing rule influence the
 performance of alpha investing?  Let $U$ denote a figure of merit, or utility,
 provided by using an alpha investing rule.  For example, the utility might be
 the expected number of hypothesis $H_j:\mu_j=0$ rejected by the alpha investing
 rule defined by the distribution $f \in {\cal F}$:
 \begin{eqnarray}
    U_r(\mu_{1:T},\,f) 
      &=& \ev_{\mu_{1:T}} \sum_{j=1}^T r_{\mu_j}(\alpha_j) \label{eq:Ur} \\
      &=& r_{\mu_1}(\al_1) \left( 1 + r_{\mu_2}(W_1-\al_1+\omega) + \cdots \right)\cr
      & &  + (1-r_{\mu_1}(\al_1))\left( r_{\mu_2}(W_1-\al_1) + \cdots \right) \;,
 \label{eq:Ure}
 \end{eqnarray}
 where $r_\mu(\al) = \Phi(\mu - z_\alpha)$ is the probability of rejecting,
 $z_\al = \Phi^{-1}(1-\al)$ is the normal quantile, and $\Phi$ is the cumulative
 standard normal distribution.  In \eqn{eq:Ur}, $\al_j$ denotes the amount
 invested in the test of $H_j$ by following the investing rule defined by the
 distribution $f$; this notation suppresses the detail of how prior rejections
 influence this random variable as suggested by \eqn{eq:Ure} that shows how
 $\al_2$ depends on the prior outcome.  Alternatively, we also measure the
 utility in the sense of accumulated (negative) risk.  If $Z \sim N(\mu,1)$,
 then the risk of the testimator $\hat\mu = Z \,I_{\{Z < z_\al\}}$ is
 \begin{equation}
   R_{\mu}(\al) = (1-r_\mu(\al))\mu^2 + (z_\al-\mu)\phi(z_\al-\mu) + \Phi(\mu-z_\al)
 \label{eq:Rmu}
 \end{equation}
 The associated cumulative utility is then
 \begin{equation}
    U_R(\mu_{1:T},\,f) = \ev_{\mu_{1:T}} \sum_{j=1}^T R_{\mu_j}(\alpha_j) 
 \label{eq:UR}
 \end{equation}
 

 To compare two alpha investing rules defined by $f,\,g \in {\cal F}$, define
 the {\em performance envelope} of $(f,\,g)$ to be the region
 \begin{equation}
    \mbox{PE}(f,g;\rho) = \{(x,\,y) \in \R^2: \exists \; \mu_{1:T} \in \RT \; s.t. \; 
                     x = U_\rho(\mu_{1:T},f),\,  y=U_\rho(\mu_{1:T},g))\} \;.
 \label{eq:PE}
 \end{equation}
 The point $(x,\,y)$ lies in the performance envelope if there exists a sequence
 of means for which these coordinates identify the utilities obtained by the two
 alpha investing rules.  As an example, Figure \eqn{fi:pe} shows PE$(g_{0.11},u;\;
 r)$, the rejection performance envelope of alpha investing with a geometric
 distribution having $\psi = 0.11$ versus the universal distribution $u$ defined
 in \eqn{eq:univ}.  Points within the performance envelope of $(g_{0.11},u)$ that lie
 below the diagonal indicate parameters $\mu_{1:T}$ for which $g_{0.11}$ produces
 higher utility than $u$; those above the diagonal (the larger portion of Figure
 \eqn{fi:pe}) indicate $u$ dominates $g_{0.11}$. In this example, the universal
 distribution dominates the geometric almost everywhere.  The advantage is
 particularly stark near the origin; in this `nearly black' situation, few
 hypotheses are rejected and the universal rule produces far better performance.


 \begin{figure}
 \caption{ \label{fi:pe} Performance envelope PE$(g_{0.11},u;\;r)$ shows the
 expected number of rejected hypotheses obtained by the geometric alpha
 investing rule $g_{0.11}$ versus the universal investing rule ($T=250$)}
 \centerline{ \includegraphics[width=4in]{envelope} }
 \end{figure}


 {\ras Connection to risk inflation.}

 The risk inflation of the testimator
 $\hat\mu_{\hat\gamma}$ is the maximum over choices of $\mu$ (and hence of
 $\gamma$) of the ratio of the risk of $\hat\mu_{\hat\gamma}$ to the smallest
 risk attainable by any testimator:
 \begin{equation}
    \mbox{RI}(\hat\gamma) 
          = \sup_\mu  \frac{R(\mu, \hat\mu_{\hat\gamma})}
                           {\inf_\eta{R(\mu, \hat\mu_\eta)}}   \;,
 \label{eq:ri}
 \end{equation}
 where $\eta \in \{0,1\}^p$ denotes an arbitrary selector.  This version of risk
 inflation is denoted $\widetilde{\mbox{RI}}$ in \citet{fostergeorge94}; the
 more common form restricts the denominator in \eqn{eq:ri} to the risk of the
 ``correct'' testimator, $R(\mu, \hat\mu_\gamma)$.  In that case, the
 denominator reduces to the number of non-zero means, $\sum \gamma_j$.  Foster
 and George show that the properties of these two definitions of risk inflation
 are similar; the version in \eqn{eq:ri} is more natural for our application.
  Given independent simultaneous (rather than sequential) tests,
 RI$(\hat\gamma)$ is asymptotically equal to $2 \log p$.  \citet{fostergeorge94}
 show that
 \begin{equation}
     2 \log p + o(\log p) \le  RI(\hat\gamma) \;,
 \label{eq:lower}
 \end{equation}
 and that hard thresholding (basically, Bonferroni selection) is essentially optimal,
 \begin{equation}
       RI(\gamma_{2 \log p}) < 1 + 2 \log p \;.
 \label{eq:upper}
 \end{equation}
 Comparable results were obtained by \citet{donohojohnstone94} around the same time. 



%--------------------------------------------------------------------------
\section{ Discussion }
%--------------------------------------------------------------------------


 Return to model selection.

  Suppose one has a collection of $p$ variables $X_1, \ldots, X_p$ to consider as
 explanatory variables in the classical linear regression model
 \begin{equation}
   Y_i = \beta_0 + \beta_1 X_{i1} + \cdots + \beta_p X_{ip} + \ep_i, 
     \qquad \ev \ep_i = 0, \Var(\ep_i)=\sigma^2,  \quad i = 1,\ldots,n\;.
 \label{eq:regr}
 \end{equation}
 As a method for picking a model, subset selection (sometimes called $L_0$
 selection) in effect tests the hypotheses $H_j: \beta_j = 0, \; j = 0, 1, \ldots, p$.
  Let $\gamma_j = \pm 1$ denote those $\beta_j \ne 0$, and let $\hat\gamma_j =
 \pm 1$ identify the rejected hypotheses.  The explanatory variable $X_j$
 appears in the fitted model if $\hat\gamma_j = 1$ and is excluded otherwise
 (hence estimating $\beta_j$ = 0).  We insert an intercept in all models (as
 needed by risk inflation below) and so set $\gamma_0=\hat\gamma_0 = 1$.  The
 vectors $\gamma = (1, \gamma_1, \ldots, \gamma_p)'$ and $\hat\gamma = (1,
 \hat\gamma_1, \ldots, \hat\gamma_p)'$ collect these indicators.


Alpha investing can mimic regular testing procedure by revisiting the test of
prior hypotheses. 

Improve the estimator by shrinkage.



One might also use accumulated alpha wealth as a measure of the performance, and
 this provides a more useful metric of performance, particularly when competing
 against the oracle.  If maximizing alpha wealth, then the oracle loses the
 amount bid and chooses $\mu_j$ to maximize $r_{\mu_j}(\al)-\al$ rather than
 $r_{\mu_j}(\al)$ alone.  This perspective would not only capture aspects of
 rejecting hypotheses, it also anticipates having resources to test future
 hypotheses.  Such consideration is appropriate, however, only in the context of
 testing a larger collection of hypotheses than considered here.

 \ras{ Things left to do:
 \begin{enumerate}
 \item Graphs of envelope that suggest that alpha wealth is a decent proxy for
 risk, at least better than something like FDR, number rejects - constant times
 number false rejects.
 \item What does the steady state look like.  If take the envelope for 250 and
 double to get for 500, is that close to correct for the risk?
 \item Comment on the value of saving if hope to compete with a universal
 bidder.
\end{enumerate}
}


%--------------------------------------------------------------------------
\section{ Bibliography }
\section*{Acknowledgement}
%--------------------------------------------------------------------------

The authors thank ...


%--------------------------------------------------------------------------
% References
%--------------------------------------------------------------------------

\bibliography{../../../../references/stat}
\bibliographystyle{../../bst/asa}

\end{document} %==========================================================
