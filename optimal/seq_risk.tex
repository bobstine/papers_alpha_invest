% -*- mode: LaTex; outline-regexp: "\\\\section\\|\\\\subsection";fill-column: 80; -*-
\documentclass[12pt]{article}
\usepackage[longnamesfirst]{natbib}
\usepackage[usenames]{color}
\usepackage{graphicx}  % Macintosh pdf files for figures
\usepackage{bbm}       % one symbol
\usepackage{amssymb}   % Real number symbol {\Bbb R}
\input{../../standard}

% --- margins
\usepackage{../../sty/simplemargins}
\setleftmargin{1in}   % 1 inch is NSF legal minimum
\setrightmargin{1in}  % 1 inch is NSF legal minimum
\settopmargin{1in}    % 1 inch is NSF legal minimum
\setbottommargin{1in} % 1 inch is NSF legal minimum

% --- Paragraph split, indents
\setlength{\parskip}{0.00in}
\setlength{\parindent}{0in}

% --- Line spacing
\renewcommand{\baselinestretch}{1.3}

% --- Margins
\setlength{\topmargin}{-0.5in}
\setlength{\oddsidemargin}{-0.1in}
\setlength{\textheight}{9.0in}
\setlength{\textwidth}{6.5in}

% --- page numbers
\pagestyle{empty}  % so no page numbers

% --- Hypthenation
\sloppy  % fewer hyphenated
\hyphenation{stan-dard}
\hyphenation{among}

% --- Customized commands, abbreviations
\newcommand{\TIT}{{\it  {\tiny Risk of sequential estimators (\today)}}}

\newcommand{\test}{\mbox{$\hat\mu(\al(\cdot),w_0,\omega)$}}
\newcommand{\uTest}{\mbox{$\hat\mu(\al_u(\cdot),w_0,\omega)$}}
\newcommand{\gTest}[1]{\mbox{$\hat\mu(\al_g(\cdot,{#1}),w_0,\omega)$}}

% --- Header
\pagestyle{myheadings}
\markright{\TIT}

% --- Title

\title{ Risk of Sequential Estimators \\ Defined by Alpha Investing }
\author{
        Dean P. Foster and Robert A. Stine
        \thanks{Research supported by NSF grant DMS-1106743 }  \\
        Department of Statistics            \\
        The Wharton School of the University of Pennsylvania \\
        Philadelphia, PA 19104-6340                          \\
        www-stat.wharton.upenn.edu/$\sim$stine 
}

\date{\today}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle 
%------------------------------------------------------------------------

\abstract{ 

 Streaming feature selection evaluates potential explanatory variables
 sequentially rather than all at once.  This approach provides adaptive control
 of the scope of a search for predictive features and makes it possible to
 explore large collections of features.  It also produces novel challenges for
 variable selection.  Methods such as alpha investing can be used to control the
 rate of false discoveries, but little is known about the risk of the resulting
 estimator.  We provide a computational framework based on stochastic dynamic
 programming that determines the exact, nonasymptotic minimax risk of a
 sequential estimator relative to an alternative.  The alternative can be data
 driven or derived from an oracle.  We illustrate the power of this framework by
 computing the risk inflation of sequential estimators derived from several
 alpha investing rules.  We find that a universal investing rule performs well
 and that estimators allowed to have larger than conventional rates of false
 discoveries produce generally smaller risk.

}

%------------------------------------------------------------------------
\vspace{0.05in}

\noindent
{\it Key Phrases: Bellman equations, stochastic dynamic programming, streaming
 feature selection, testimator, variable selection}

\clearpage


% ----------------------------------------------------------------------
\section{ Introduction }
% ----------------------------------------------------------------------

 Our analysis concerns the risk of sequential estimators in which
 characteristics of later estimates depend on the outcomes of earlier
 estimates.  Our interest in the risk of such estimators arises from their use
 in streaming feature selection.  Streaming feature selection constructs a
 predictive model by choosing explanatory variables from a sequence offered by
 an exogenous source.  Think of forward stepwise regression, but without
 knowledge of the complete domain of explanatory variables.  Rather than evaluate
 these possible explanatory variables simultaneously, streaming selection
 evaluates them one-at-a-time.  Greedy searches like stepwise regression consider
 the full batch of, say, $p$ potential explanatory variables together, choosing
 at the first step the predictor $X_{(1)}$ that obtains the best fitting model.
  In contrast, streaming selection evaluates the offered predictors sequentially
 as $X_1, \, X_2, \ldots$, evaluating $X_j$ in the context of
 the model produced by picking features from $X_1, \ldots, X_{j-1}$.  Hence,
 streaming selection does not require the full set of explanatory variables at
 the start of the search and is free to use the results of evaluating initial
 variables to guide the search for those to consider subsequently.  For example,
 a streaming search might add the interaction $X_j \, X_k$ to the queue of
 possible features after finding significant effects for $X_j$ and $X_k$.
  Streaming selection can thus adaptively explore collections of explanatory
 variables that are larger than typically considered with conventional methods;
 for large samples, the slowest step in forward stepwise regression is the
 calculation of the $X'X$ matrix.  \citet{fosterlin11} demonstrate the speed
 attained by sequential selection when picking a regression from
 $p$=100,000 explanatory variables.


 Streaming selection poses a challenge, however, for variable selection.
  Although one gains advantages by avoiding simultaneously evaluating every
 predictor, the absence of a fixed set of features in streaming selection
 requires a different type of selection criterion from those commonly used.  For
 example, suppose the search begins with a list of $p$ possible features $X_1,
 X_2, \ldots, X_p$.  As mentioned previously, the search could expand to include
 interactions in $X_j$ once $X_j$ joins the model.  If the search is limited to
 second-order interactions (one could allow higher order interactions as well),
 then the maximum number of possible explanatory variables is $m = p(p+1)/2$
 variables.  Since few of these would be considered, it would be very
 conservative to combine $m$ with a criterion such as AIC, BIC, or RIC.
  Similarly, selection using FDR requires the complete set of $p$-values at the
 start of the search.


 Alpha investing \citep{fosterstine08} is a sequential testing procedure that we
 designed to work with streaming feature selection.  Because alpha investing can
 test an infinite sequence of hypotheses, it is well-matched to a search of a
 possibly unbounded collection of features that is too large to manipulate
 simultaneously.  Rather than test multiple hypotheses at once, alpha investing
 tests hypotheses one-at-a-time in a specified order.  Alpha investing begins
 with an initial allowance for Type I error that is called its alpha wealth.
  Each test consumes some of the available alpha wealth, as in alpha spending
 rules used in clinical trials.  Alpha investing overcomes the conservatism of
 alpha spending rules, which include the Bonferroni method, by earning a
 contribution to the alpha wealth for each rejected null hypothesis.  Thus
 rejections beget more rejections.  Alpha investing further allows one to test
 an infinite stream of hypotheses, accommodate dependent tests, and incorporate
 domain knowledge.
 

 Like other procedures for multiple testing, alpha investing controls the
 expected number of false rejections.  Controlling the false discovery rate
 protects against overfitting in variable selection.  One can guarantee that on
 average not more than, say, 5\% of the rejected hypotheses spuriously add a
 predictor to the model.  When building a predictive model, however, we find
 that controlling the false discovery rate is secondary to obtaining a more
 predictive model.  Control of the false discovery rate does not imply that one
 finds the most predictive model possible.  It only guarantees that a high
 percentage of chosen features are in fact useful.  The quadratic risk of the
 implied estimator is more relevant.


 The analysis developed here finds the cumulative risk of a sequence of testimators implied by
 testing a sequence of null hypotheses.  A testimator is also known as a
 keep-or-kill estimator or a hard thresholding estimator.  The estimator of the
 parameter $\mu$ is zero unless a test of $H_0: \; \mu = 0$ is rejected.  For
 this paper, we consider a simplified version of the variable selection problem
 that avoids issues related to collinearity among explanatory variables.  Rather
 than observe a sequence of slope estimates, we assume that the observed data
 are a sequence of $p$ random variables $Y_j \sim N(\mu_j,1)$.  Within
 this context, our computations identify the possible risks produced by a
 testimator for {\em any} choice of parameters.  We accomplish this task by
 adopting the perspective of risk inflation and comparing the competitive
 performance of two sequential estimators.  For any pair, we find the set of
 risks attainable by any choices of the underlying mean parameters.  Given the
 sequential nature of the problem, it should not be surprising that we rely on
 stochastic dynamic programming.  The risks so obtained are exact (up to
 computational accuracy) rather than asymptotic.  We further show a
 probabilistic model for the underlying parameters that produces those risks.
  Although we consider a sequential estimators, we find that bounds for the risk
 inflation of conventional testimators are suggestive of the finite sample
 performance of the testimators.


 The following section provides a further introduction to alpha investing.  We
 define two alpha investing strategies derived from continuous probability
 distributions.  Our construction of these strategies in Section 2 is novel and
 allows us to minimize the state space required in the dynamic program.  Section
 3 describes the risk of a sequence of testimators.  Section 4 defines the
 feasible set of all possible risks and offers examples of these that compare
 testimators to an oracle and to each other.  Section 5 describes the
 computations in more detail.  We conclude in Section 6 with a brief discussion
 of the results and pose conjectures motivated by our computations.



% ---------------------------------------------------------------------------
\section{ Alpha-investing }
% ---------------------------------------------------------------------------

 An alpha-investing rule \citep{fosterstine08} determines the levels for testing
 a sequence of hypotheses $H_1, \,H_2, \dots,$.  The procedure is most easily described
 by showing a few steps.  The process begins with an initial allocation $w_0 >
 0$ of alpha wealth.    An alpha-investing rule can test $H_1$ at
 any level $\al_1$ up to the initial alpha wealth, $0 \le \alpha_1 \le w_0$.
  The level $\al_1$ is `invested' and cannot be used for subsequent tests.  We
 say that $\al_1$ is invested rather than spent because  the
 outcome of testing $H_1$ increases the alpha wealth if $H_1$ is rejected.  Let $p_1$ denote
 the p-value of the test of $H_1$.  If $p_1 \le \alpha_1$, the test rejects
 $H_1$, and the alpha investing rule earns a contribution $\omega \ge
 0$ to its alpha wealth.  Otherwise, the alpha wealth available to test $H_2$
 falls to $W_1 = w_0 - \alpha_1$.  In general, the alpha wealth available for
 testing $H_{j+1}$ is given by the stochastic process
 \begin{equation}
    W_j = W_{j-1} - \alpha_j + \omega \, I_{\{p_j \le \al_j\}}, 
       \quad j = 1,\,2,\ldots,
 \label{eq:Wj}
 \end{equation}
 with the initial condition $W_0 \equiv w_0$.  Alpha spending rules are alpha
 investing which fix $\omega = 0$.


 Because rejecting a null hypothesis makes it easier to reject other null
 hypotheses, it is essential for alpha investing to control the rate of false
 rejections.  To this end, \citet{fosterstine08} show that alpha investing
 controls a sequential version of the expected false discovery rate, mFDR.  Let
 $T(j)$ count the number of hypothesis rejected in the first $j$ tests, and let
 $V(j) \le T(j)$ denote the total number of {\em false} rejections among the
 first $j$ tests.  The sequential mFDR is
 \begin{equation}
    \mbox{mFDR}_\eta(j) = \frac{\ev V(j)}{\eta + \ev T(j)} \;, \eta > 0.
 \label{eq:mFDR}
 \end{equation}
 In contrast, FDR is the expected value of the ratio $V(j)/T(j)$ conditional on
 $T(j)>0$ rather than the ratio of expected values.  The constant $\eta$ in the
 denominator of mFDR avoids dividing by values near zero under the complete null
 hypothesis in which all $\mu_j = 0$.  If $w_0 \le \eta\,\omega$, then
 alpha-investing rules control $\mbox{mFDR}_\eta(p) \le \omega$, and this result
 implies weak control of the family wide error rate.  Because these properties
 of alpha investing originate in a martingale, the index $j$ in \eqn{eq:mFDR} is
 allowed to be an arbitrary stopping time, such as the occurrence of the $k$th
 rejection.

 
 Alpha-investing rules are quite general.  The underlying theory requires only
 that the level of the test of $H_j$ is bounded by the available wealth
 $W_{j-1}$ and that the test indeed have level $\al_j$, conditional on the
 outcomes of prior tests.  Otherwise, an alpha investing rule can use the
 pattern of prior rejections captured by the sequence of wealths.  We represent
 this dependence by writing an investing rule ${\cal A}$ as a function of the
 sequence of prior wealths $W_{0:j-1} = \{W_0, W_1, \ldots, W_{j-1}\}$.  The rule
 $\cal A$ maps this history to the interval from zero to the current wealth:
 \begin{equation}
    {\cal A}: W_{0:j-1} \mapsto [0, \min(W_{j-1},1)]    
 \label{eq:A}
 \end{equation}
 For example, $\cal A$ can set the level of the next test higher or lower
 depending upon the number of tests since the last rejection or the number of
 rejections accumulated so far.  Although alpha investing allows this
 generality, we focus on a simpler class of investing rules that have a path
 independent, Markovian structure.  The amount invested by these rules depends
 only on the current wealth rather than the full path, $\alpha_j = {\cal
 A}(W_{0:j-1}) = \al(W_{j-1})$.  The only requirement is that $0 \le \al(w) \le w$; a
 rule cannot invest more wealth than the amount possessed.  It does seem
 natural, however, for $\al(w)$ to be monotone increasing in $w$.


 \remark{ To avoid adding notation, we overload the symbol $\al$.  Throughout
 these uses, the symbol $\alpha$ consistently gives the level of a test; only
 the context of the test changes.  By itself, we use $\al$ to represent the
 generic level of a test.  When given an integer subscript, $\al_j$ is the level
 of the $j$th test in sequence of tests, as in \eqn{eq:Wj}.  Finally, denoting a
 function, $\al(w)$ is the level invested in a test by an alpha investing rule
 that has available wealth $w$, as in \eqn{eq:Ageo} that follows.  }


 The simplest representatives of this class of alpha investing rules are
 geometric rules.  These invest a fixed percentage of the available wealth on
 each test:
 \begin{equation}
    \al_g(w, \psi) = \psi \, w, \quad  0 < \psi < 1.
 \label{eq:Ageo}
 \end{equation}
 Since the alpha wealth increases after a rejection, a geometric rule $\al_g$
 invests more following a rejection and then gradually -- at the rate determined
 by $\psi$ -- reduces the level of subsequent tests.


 Alternatively an alpha investing rule can vary the share of the current wealth
 to invest in the next test.  Rather than invest a fixed share of the available
 wealth, the following rule invests progressively less wealth as the wealth
 drops.  The rule is defined by
 \begin{equation}
   \al_u(w) = w - \frac{\log 2}{\log(1+2^{1/w})} \;.   
 \label{eq:Auniv}
 \end{equation}
 Because we obtained this investing rule from the universal prior for integers
 defined by \citet{rissanen83}, we call this a universal investing rule and
 identify it by a subscript $u$.  (See the following remark.)

 
 Figure \ref{fig:rules} contrasts the investments produced by the universal and
 several geometric alpha investing rules on a log-log scale.  The figure conveys
 the sense in which $\al_u$ is universal in the way that it mimics a collection
 of geometric rules.  For convenience, the initial wealth for all rules shown in
 Figure \ref{fig:rules} is $w_0 = 1$.  With this scaling, the amounts invested
 by the universal rule fall off approximately linearly.  The amounts are
 initially larger than those of any of the geometric rules.  Starting from
 $w_0=1$, the universal rule invests about $0.369$, $0.131$, $0.0693$, $0.0438$,
 and $0.0306$ in the first five tests before its spending gradually slows.
  After this initial period, each geometric rule provides a larger alpha level
 than the universal rule for a range of tests.  Ultimately, however, the
 universal rule invests more than every geometric rule.  For example, the
 geometric rule that invests 1\% of its wealth at each test ($\psi=0.01$)
 invests more than the universal rule when testing $H_{11}$ through $H_{581}$;
 otherwise it invests less.  Each geometric spending rule with successively
 smaller rate $\psi$ eventually spends more than the rules that spend at a
 larger rate; eventually each rule spends the most for a portion of testing.
  The graph shows that the universal rule saves enough so that it too can spend
 close to this rate.  Making this precise is exactly what the makes it the
 underlying distribution universal in the information theoretic sense.

 % Too bad we don't have that 'habitation' idea worked out where the universal
 % finds a steady rate appropriate for the rate of false nulls.


 \begin{figure}
 \caption{ \label{fig:rules} { \sl Investments for the universal and several
 geometric alpha investing rules if no intervening test rejects. } The initial
 wealth $w_0$=1. }

 \centerline{
 \vspace{0.1in}
 \includegraphics[width=3.5in]{figures/rules} }
 \vspace{0.2in}
 \end{figure}


 \remark{ We obtained the universal rule $\al_u$ defined in \eqn{eq:Auniv} by
 the following construction.  The idea is to define a spending rule by a
 probability distribution that allocates wealth over subsequent tests and then
 shift from discrete to continuous distributions.  We illustrate the
 construction for the geometric rule.  If the initial wealth is $w_0$, then the
 alpha wealth invested in the $j$th test (assuming no intervening test rejects)
 is
 \begin{equation}
    \al_j = w_0\; \psi \; (1-\psi)^{j-1} \;, \quad j = 1, 2, \ldots\;.   
 \label{eq:alj1}
 \end{equation}
  Rather than define the investing rule using a discrete distribution on
 $j=1,\,2,\ldots$ as in \eqn{eq:alj1}, consider the continuous density
 \begin{equation}
   A_g(x) = c_g \; \psi\,(1-\psi)^{x-1}, \quad 1 \le x\;,
 \label{eq:alg}
 \end{equation}
 where the normalizing constant $c_g = -\log (1-\psi)/\psi$ implies
 $\int_1^{\infty} A_g(x) dx = 1$.  Notice that the wealth invested in the $j$th
 test \eqn{eq:alj1} matches $w_0$ times the integral of $A_g(x)$ from $x=j$ to
 $j+1$,
 \begin{equation}
    \al_j = w_0 \int_j^{j+1} A_g(x) dx = w_0 \, \psi\; (1-\psi)^{j-1} \;.
 \label{eq:alj2}
 \end{equation}
 To move away from discrete indexing, we use the following tail integral,
 \begin{equation}
    W_g(x) = w_0 \int_x^\infty A_g(t,\psi) dt = w_0 (1-\psi)^{x-1}\;.
 \label{eq:Wg}
 \end{equation}
 For integers $j$, $W_g(j)$ is the wealth available to test $H_j$ if none of
 $H_1$, $H_2, \ldots,\, H_{j-1}$ are rejected.  By inverting this tail integral,
 we can write the investing rule as a function of just the available wealth,
 \begin{equation}
   \al_g(w) = A_g(W_g^{-1}(w)) = \psi\,w \;,
 \label{eq:Ag}
 \end{equation}
 as in \eqn{eq:Ageo}.  This construction uses the inverse of the tail wealth to
 determine a `hypothesis index' $W_g^{-1}(w)$ that corresponds to wealth $w$.
  The universal rule $\al_u(w)$ follows from the same construction, but starts
 with the density
 \begin{displaymath}
    A_u(x) = \frac{\log 2}{(x+1) (\log(x+1))^2} \;, \quad 1 < x,
 \end{displaymath}
 in place of $A_g$.  $A_u(x)$ is a continuous version of the distribution
 defined by the penultimate code of \citet{elias75}.  The universal code of
 \citet{rissanen83} comes from a more further refinement of the penultimate
 code. }


%--------------------------------------------------------------------------
\section{ Risk Analysis}
%--------------------------------------------------------------------------


 Before turning to sequential estimators, we briefly review the quadratic risk
 of testimators.  We begin with the scalar case.  Let $Y \sim N(\mu,1)$.  The
 scalar testimator defined by the two-sided test of $H_0: \mu=0$ at level $\al$
 is
 \begin{equation}
   \hat\mu_\al(Y) = \left\{
     \begin{array}{cc}
        Y & \mbox{ if } z_\al^2 \le Y^2, \cr
        0 & \mbox{ otherwise, }
      \end{array} \right.
 \label{eq:muHat}
 \end{equation}
 where $z_\al$ denotes the positive, two-sided critical value
 \begin{equation}
   z_\al = \Phi^{-1}(1-\al/2) \;.
 \label{eq:zAlpha}
 \end{equation}
 The risk of $\hat\mu_\al$ is
 \begin{eqnarray}
   R(\hat\mu_\al(Y), \mu) 
     &=& \ev(\hat\mu_\al(Y) - \mu)^2  \cr
     &=& \mu^2 \pr(Y^2 \le z_\al^2) 
         + \int_{z_\al^2<y^2} (y-\mu)^2 \phi(y) dy \cr
     &=& B_\al(\mu) + V_\al(\mu)
 \label{eq:risk_mu_al}
 \end{eqnarray}
 The first summand $B_\al(\mu)$ is the squared bias that arises if the test of
 $H_0: \mu=0$ does not reject when $\mu \ne 0$; the second summand is the
 variance of the estimator.  Figure \ref{fig:risk}(a) shows a graph of the risk
 of testimators with $\al=0.05$ and $\al = 0.20$.  The maximum risk occurs
 near $z_\al$ and grows as the level $\al$ shrinks.  Figure \ref{fig:risk}(b) shows
 the decomposition of the risk of $\hat\mu_\al$ into $B_\al(\mu)$ and
 $V_\al(\mu)$ for $\al = 0.05$.  Because the variance component $V_\al(\mu)$
 increases smoothly to its maximum 1 for large $|\mu|$, it is the bias that
 produces the noticeable peak in the risk.


 \begin{figure}
 \caption{ \label{fig:risk} \sl Risk of testimators. (a) Risk of testimators with
 $\al$ = 0.05, 0.20 versus $\mu$. (b) Squared bias and variance components of the
 risk of $\hat\mu_{0.05}$. } 
 \vspace{0.1in}
\centerline{
 \includegraphics[width=3.0in]{figures/risk_a}
 \includegraphics[width=3.0in]{figures/risk_b} }
 \vspace{0.2in}
 \end{figure}
 

 The analysis of the risk of testimators is typically done in the context of
 estimation a vector of $p$ means, $\bmu \equiv \mu_{1:p} =
 (\mu_1, \,\ldots, \mu_p)'$.  The available data is the vector $\YY \equiv Y_{1:p} =
 (Y_1, \ldots, Y_p)'$ with distribution $\YY \sim N(\bmu,I_p)$.  The estimator
 of $\bmu$ combines testimators with a common level, $\hat{\bmu}_\al =
 (\hat\mu_\al(Y_1), \ldots, \hat\mu_\al(Y_p))'$.  The estimator $\hat{\bmu}_\al$
 consists of zeros except for those coordinates where $z_\al^2 \le Y_j^2$.  The
 risk of $\hat{\bmu}_\al$ is the sum of the risks of the coordinate testimators,
 \begin{equation}
    R(\hat{\bmu}_\al, \bmu) 
      = \ev \sum_{j=1}^p \Bigl( \hat{\mu}_{\al}(Y_j) - \mu_j \Bigr)^2 \;.
 \label{eq:risk}
 \end{equation}


 Minimax bounds for the risk $R(\hat{\bmu}_\al,\bmu)$ are well-understood.  We
 review the results of \citet{fostergeorge94} who introduced the concept of the
 risk inflation of an estimator. \citep[][obtain similar results.]
  {donohojohnstone94} The risk inflation of $\hat{\bmu}_\al$ is the supremum of
 the ratio of the risk of $\hat{\bmu}_\al$ to that of a testimator that obtains
 the optimal level from an oracle.  Their results imply that the risk inflation
 of $\hat{\bmu}_\al$ is asymptotically about $2 \log p$,
 \begin{equation}
    2 \log p - o(\log p) 
    \le
    \sup_\mu  \frac{1 + R(\hat{\bmu}_\al,\bmu)}
                   {1 + \inf_\eta{R(\hat{\bmu}_\eta,\bmu)}}  
    \le 
    2 \log p + 1 \;.
 \label{eq:ri}
 \end{equation}
 Foster and George further show that the testimator $\hat{\bmu}_{1/p}$ --
 essentially the Bonferroni estimator -- obtains the risk inflation threshold.
  The constant 1 added to the risks in the ratio of \eqn{eq:ri} arises in the
 context of regression models in which one always estimates the intercept.  As a
 practical device, its presence avoids dividing by zero under the complete null
 model in which $\mu_j = 0$ for all $j$.


 Though suggestive, these results do not reveal the risk of the testimator
 derived from alpha investing.  The key distinction lies in the timing of the
 information revealed in $\YY$.  The testimator $\hat{\bmu}_\al$ studied in risk
 inflation uses a fixed level $\al$ for all $p$ coordinates, and all of the
 elements of \YY are available for choosing $\al$.  In sequential testing
 controlled by alpha investing, the $Y_j$ are observed sequentially.  The
 elements of the estimator form a stochastic process, which we collect in a
 $p$-element vector as
 \begin{equation}
   \hat\mu(\al(\cdot),w_0,\omega) = (\hat\mu_{\al(w_0)}, \hat\mu_{\al(W_1)}, \ldots, 
                       \hat\mu_{\al(W_{p-1})})',
 \label{eq:muHatAlphaInv}
 \end{equation}
 where $\al(\cdot)$ denotes the defining investing rule, $w_0$ is the initial
 alpha wealth, and $\omega$ is the payout earned when rejecting a hypothesis.
  We omit $w_0$ and $\omega$ from this notation when convenient and unambiguous.
 

 The most convenient expression for the risk of \test\ relies on a recurrence.
  For convenience, let
 \begin{equation}
   r_\mu(\al) = \Phi(\mu-z_\al)+\Phi(-\mu-z_\al)   
 \label{eq:rMu}
 \end{equation}
 denote the probability of rejecting $H_0: \mu=0$ using a two-sided $z$-test at
 level $\al$ (the power of the test).  The risk of the testimator given by alpha
 investing can then be decomposed as
 \begin{eqnarray}
   R(\hat\mu(\al(\cdot),w_0,\omega),\mu_{1:p}) 
    &=& R(\hat\mu_{\al(w_0)},\mu_1)
        + \ev \sum_{j=2}^p R\Bigl(\hat\mu_{\al(W_{j-1})},\mu_j\Bigr)  \cr
    &=& R(\hat\mu_{\al_1},\mu_1)
        + r_{\mu_1}(\al_1)\; 
          R\Bigl(\hat\mu(\al(\cdot),w_0-\al_1+\omega,\omega),\mu_{2:p}\Bigr)\cr
    & & \qquad + (1-r_{\mu_1}(\al_1))\; 
          R\Bigl(\hat\mu(\al(\cdot),w_0-\al_1,\omega),\mu_{2:p}\Bigr) \;,
 \label{eq:RMuHat}
 \end{eqnarray}
 where $\al_1 = \al(w_0)$ and we have suppressed the dependence of the estimator
 on \YY.  The second expression for the risk emphasizes its recursive nature and
 motivates our method of computation.  The total risk of the estimator is the
 risk produced by the testimator for $H_1$ plus the cumulative risk of testing
 $H_2, \ldots, H_p$.  If the test of $H_1$ rejects, which happens with
 probability $r_{\mu_1}(\al_1)$, then testing the remaining hypotheses begins
 with alpha wealth $W_1 = w_0 - \alpha_1 + \omega$.  Otherwise, with probability
 $1- r_{\mu_1}(\al_1)$, testing begins with wealth $w_1 - \al_1$.


 The calculation of the maximum risk obtained by \test\ is similarly recursive,
 and we compute the sum by backward induction.  Because the performance of
 subsequent tests depends on the outcome of the first, the choice of $\mu_1$ is
 not so simple as setting $\mu_1 = \arg \max R(\hat\mu_{0.05}, \mu)$.  Doing so
 ignores the payoff $\omega$ obtained if $H_1$ is rejected.  By rejecting the
 first test, the alpha investing rule obtains the additional contribution
 $\omega$ to its alpha wealth, allowing it to increase the level -- and so
 potentially reduce its risk -- in subsequent tests.  The problem to be solved
 at the first test is to choose
 \begin{eqnarray*}
    \mu_1 = \arg \max_{m} & \Bigl\{ & R(\hat\mu_{\al_1},m) \Bigr. 
        + r_{m}(\al_1) \; \max_{\mu_{2:p}} 
              R(\hat\mu(\al,w_0-\al_1+\omega,\omega),\mu_{2:p})\cr
    & & + \Bigl. (1-r_{m}(\al_1)) \max_{\mu_{2:p}} \; 
              R(\hat\mu(\al,w_0-\al_1,\omega),\mu_{2:p}) \Bigr\} \;.
 \end{eqnarray*}
 Notice that the means $\mu_1,\, \mu_2,\ldots, \mu_p$ that maximize the risk are
 not deterministic because of the stochastic outcome of the tests.  As a result,
 our calculations define a stochastic process for the mean that obtains, on
 average, the maximum risk.


%--------------------------------------------------------------------------
\section{ Feasible Risk Set }
%--------------------------------------------------------------------------


 Our interest is not simply in the risk of a testimator, however, but in its
 risk when compared to an alternative.  We want to see how various sequential
 testimators perform when estimating the same collection of means.  In the style
 of risk inflation \eqn{eq:ri}, we want to contrast the risk of \test\ to that
 of another realizable testimator or to a testimator that benefits from an
 oracle that reveals $\bmu$.  The oracle-based, risk-inflation testimator
 $\tilde{\bmu}$ has elements
 \begin{equation}
   \tilde\mu_j(Y_j) = \left\{ \begin{array}{cc} 
                       0    & \mbox{ if } \mu_j^2 \le 1,        \cr
                       Y_j  & \mbox{ otherwise. }
                \end{array} \right.
 \label{eq:muTilde}
 \end{equation}
 so that its risk is 
 \begin{equation}
    R(\tilde{\bmu},\bmu) = \sum_j \min(\mu_j^2,1) \;.   
 \label{eq:riskMuTilde}
 \end{equation}
 We summarize such comparisons of risks by finding the collection of all
 possible risks that are obtainable under any mean process.  We call this
 collection the feasible risk set.  Let $\hat{\bmu}_1$ and $\hat{\bmu}_2$ denote
 two sequential estimators of $\mu_{1:p}$.  The feasible risk set for these two
 is defined as
 \begin{equation}
     {\cal R}_p(\hat{\bmu}_1,\hat{\bmu}_2) = 
      \{(r_1,r_2):  \exists \bmu \mbox{  s.t.  }
          r_1 = \evsub{\bmu} R(\hat{\bmu}_1,\bmu),\;
          r_2 = \evsub{\bmu} R(\hat{\bmu}_2,\bmu)  \} \;.           
 \label{eq:feasibleSet}
 \end{equation}
 In words, the point $(r_1,\,r_2)$ lies in the feasible set ${\cal R}_p$ if
 there exists a stochastic process of means $\bmu$ of length $p$ for which the
 risk of $\hat{\bmu}_1$ is $r_1$ and the risk of $\hat{\bmu}_2$ is $r_2$.  A
 randomization argument proves that the feasible risk set is convex.  If $x$ and
 $y$ are two points within ${\cal R}_p$, then there exist stochastic processes
 $\bmu_x$ and $\bmu_y$, say, that produce these risks.  The risk produced by the
 randomized process that picks $\bmu_x$ with probability $0 \le a \le 1$ and
 picks $\bmu_y$ with probability $1-a$ is then $a\,x+(1-a)\,y$.


 Figure \ref{fig:riFeasibleSet} shows two views of the feasible set that
 compares the oracle testimator $\tilde{\bmu}$ ($x$-axis) to the universal
 testimator \uTest\ ($y$-axis).  For this figure, $p$=1,000 tests and the
 initial wealth and payout $w_0 = \omega = 0.5$.  The feasible risk set is the
 shaded region in each frame.  The feasible risk set lies above the diagonal in
 this comparison; by construction, no realizable testimator can have smaller
 risk than $\tilde{\bmu}$.  The frame on the left of Figure
 \ref{fig:riFeasibleSet} shows the feasible set on the scale of risks; the frame
 on the right shows the same data on log scales.  (The feasible risk set is not
 convex on a log scale but the approximation is quite close in practice.)  We
 add 1 to the risks of both estimators, in the fashion of risk inflation, in
 order to be able to show the feasible risk set near 0 on a log-log scale.
  Points in the plot along the boundary of the feasible set identify locations
 at which we computed the exact risks using the method described in the
 following section.  Consequently, because the shaded region in the graph is
 obtained by joining these points with lines, this region is a convex subset
 within the interior of ${\cal R}_p$.  The actual risk set is slightly larger.


 \begin{figure}
 \caption{ \label{fig:riFeasibleSet} {\sl Feasible set comparing the risk
 inflation oracle to the risk of the universal estimator \uTest with
 $w_0=\omega=0.5$ with $p$=1,000 tests on the scale of risks (left) or log risks
 (right).}  Curves within the feasible set show the risks for varying signal
 levels defined in \eqn{eq:muj}. }

 \vspace{0.1in}
\centerline{
 \includegraphics[width=3.0in]{figures/riFeasSet}
 \includegraphics[width=3.0in]{figures/riFeasSetLog} }
 \vspace{0.2in}
 \end{figure}


 The two scales of the risk shown in Figure \ref{fig:riFeasibleSet} emphasize
 models with substantial signal (non-zero means) and those that are sparse or
 nearly black (most $\mu_j=0$).  The frame scaled by the risk itself emphasizes
 the performance in models with substantial signal.  The vertical right edge of
 the feasible set shows the risk for saturated models in which $|\mu_j| \ge 1$;
 for these models, the risk of the oracle testimator is ${\cal
 R}_p(\tilde{\bmu}, \bmu) = p$.  The plot on the log scale emphasizes sparse
 models.  In this frame, the line parallel to and above the diagonal is the
 risk-inflation boundary \eqn{eq:ri} that obtains for non-sequential estimators.
  These bounds suggest that the worst case risk for the testimator should be
 about $2 \log p$ times the risk of the oracle.  The feasible set calculations
 show that the risk of \uTest\ does indeed fall below this boundary, but that is
 not true of all estimators. 


 Curves within the feasible set shown in Figure \ref{fig:riFeasibleSet} identify
 the risks that result if the means $\mu_{1:p}$ are determined by a two-point
 Bayesian model.  Suppose that the stochastic process that determines $\bmu$
 sets $\mu_j$ at random to some non-zero value $\mu^{*}$:
 \begin{equation}
    \mu_j = B_j \; \mu^{*}, \quad B_j  \iid \mbox{ Bernoulli}(\pi)  \;.
 \label{eq:muj}
 \end{equation}
 The smooth curves within the feasible set show the risks under this
 probabilistic model, with $\mu^{*}$ = 1.0 (red), 1.5 (magenta), or 3 (blue),
 and the probability of a non-zero mean varying over the range $10^{-6} \le \pi
 \le 1-10^{-6}$.  With $\mu^{*} = 1.0$, the risks nearly trace out the lower
 boundary of the feasible set.  The crossing of the paths for $\mu^{*}=1.5$ and
 $\mu^{*}=3$ shows, however, that no one value for $\mu^{*}$ can reproduce the
 upper boundary of ${\cal R}_p$.


 Figure \ref{fig:simRisk} graphs a realized example of the stochastic process
 that generates the risks on the boundary of the feasible risk set.  For this
 figure, we chose the process that produces the point with expected risks $(101,
 657)$ which can be found along the left side of Figure \ref{fig:riFeasibleSet}.
  The points in Figure \ref{fig:simRisk} graph the elements $\mu_j$ versus the
 test index $j$ for $j = 1,\,2, \ldots, p=1,000$.  The increasing trend in the
 figure shows the cumulative risk of the universal testimator for this
 particular realization.  The risk of the testimator in this example reaches 688
 whereas the the oracle accumulates risk 123.  The nonzero value for the mean is
 nearly constant and fluctuates around 2.63, though some realizations show an
 end effect as the testing nears $p=$1,000.


\begin{figure}
 \caption{ \label{fig:simRisk} {\sl Realization of the mean process that
 produces a point on the boundary of the feasible set.} The increasing trend
 shows the accumulating risk of the testimator which reaches 688 in this
 example.  }

 \vspace{0.1in}
 \centerline{
 \includegraphics[width=3.5in]{figures/simRisk}    }
 \vspace{0.2in}
\end{figure}


 Displays that combine several feasible sets allow one to compare the effects of
 various choices for $w_0$ and $\omega$.  As an example, Figure \ref{fig:univRI}
 considers the effect of lowering the initial wealth $w_0$ and payoff $\omega$
 from 0.50 down to smaller values, here 0.25 and 0.05.  As before, the left
 frame emphasizes estimation with greater levels of signal; the right frame on
 the log scale emphasizes sparse models.  Within the context of testing,
 choosing $\al=0.05$ is the virtual default and an intuitive choice would be to
 similarly choose to control the false discovery rate at 0.05.  Unless one is
 very convinced that nature will play a nearly black strategy, however, this
 choice of $w_0$ and $\omega$ generates far greater risk than $w_0=0.25$ or
 0.50.  With $w_0=0.05$, the risks even escape the bounds suggested by risk
 inflation in the non-sequential setting, shown here by points in the feasible
 set above the bound provided in \eqn{eq:ri}.  Because the plots in Figure
 \ref{fig:univRI} show several feasible sets together, one can no longer
 associate a point in the graph as representing a single mean process $\bmu$.
  Points within each feasible set for that pair of estimators indicate that
 there exists a mean process that generates the shown risks, but at a given
 $(x,y)$ location, the mean processes that produce the risks for the feasible sets
 differ.


\begin{figure}
 \caption{ \label{fig:univRI} {\sl Feasible risk sets comparing the risk
 inflation oracle to the risk of the universal estimator \uTest with $w_0=0.05$,
 0.25, and 0.50 with $p$=1,000 tests on the scale of risks (left) or log risks
 (right).}  }

 \vspace{0.1in}
 \centerline{
 \includegraphics[width=3.5in]{figures/univVsRI}
 \includegraphics[width=3.0in]{figures/univVsRILog}    }
 \vspace{0.2in}
\end{figure}


 We have emphasized universal investing as defined by $\al_u(\cdot)$, and Figure
 \ref{fig:univGeo} offers a partial explanation for our choice.  Figure
 \ref{fig:univGeo} superimposes the feasible sets obtained by geometric investing
 $\al_g(\cdot)$ defined in \eqn{eq:Ageo} versus the risk inflation testimator
 $\tilde{\bmu}$.  The results are for a sequence of $p = 500$ tests.  In
 general, increasing the spending rate $\psi$ from 0.001 up to 0.01 improves the
 geometric testimator as it shrinks the feasible set back toward the diagonal.
  The feasible sets for $\psi=0.001$, 0.005, and 0.01 progressively move toward
 the diagonal, better competing with the risk-inflation testimator.  The move to
 $\psi = 0.05$, however, goes too far.  The estimator essentially exhausts its
 alpha wealth before the testing is complete, and consequently its risk soars.
  Because this geometric estimator essentially sets $\hat\mu_j \equiv 0$ as its
 alpha wealth approaches 0, its risk exceeds the risk inflation boundary
 \eqn{eq:ri}.


\begin{figure}
 \caption{ \label{fig:univGeo} {\sl Comparison of the feasible risk sets for
 geometric investing rules defined by \eqn{eq:Ageo} with $p=500$ and rates
 $\psi=$ 0.001, 0.005, 0.01, and 0.05.}  Spending down the wealth too quickly
 with $\psi=0.05$ leads to excessive risk.  }

 \vspace{0.1in}
 \centerline{
   \includegraphics[width=3.5in]{figures/geom}     }
 \vspace{0.2in}
\end{figure}


 As a final example, feasible risk sets also allow us to directly compare
 realizable testimators produced by different methods of alpha investing.
  Rather than compare a realizable testimator to a method relying on an oracle
 as in Figure \ref{fig:univRI}, the feasible set ${\cal R}_p (\hat\mu(\al_u),
 \hat\mu(\al_g))$ shown in Figure \ref{fig:univGeo} pits these two against each
 other in a head-to-head comparison.  The initial value and payout for both are
 $w_0 = \omega = 0.25$.  The rates of the geometric strategy are set to
 $\psi=0.001$, 0.002, and 0.005.  Small rates are necessary to avoid the surge
 in risk illustrated in Figure \ref{fig:univGeo} when the geometric strategy runs
 out of wealth.  There are clearly mean processes for which either choice,
 universal or geometric, dominate the other.  That said, this figure clarifies
 the relative advantages of universal investing over geometric investing at
 several rates.  Again, a higher investing rate $\psi$ for geometric investing
 reduces risk for models with more signal, but doing so necessarily leads to
 higher risks in nearly black models.  For instance, the set in Figure
 \ref{fig:univGeo} associated with $\psi = 0.005$ reduces the bulge toward higher
 risks in the left frame, but this choice is soundly dominated by slower
 spending rates in models with fewer non-zero parameters emphasized in the right
 frame.  Universal investing avoids the need to set this somewhat arbitrary
 tuning parameter.


 \begin{figure}
 \caption{ \label{fig:univGeo} {\sl Direct comparison of universal investing
 (\ref{eq:Auniv}, $y$-axis) with geometric investing (\ref{eq:Ageo}, $x$-axis) at
 rates $\psi = 0.001$, 0.002, and 0.005 for $p=$ 1,000.}  }

 \vspace{0.1in}
 \centerline{
     \includegraphics[width=3.5in]{figures/univGeo}
     \includegraphics[width=2.9in]{figures/univGeoLog} }
 \vspace{0.2in}
 \end{figure}



%--------------------------------------------------------------------------
\section{ Computation }
%--------------------------------------------------------------------------

 We describe first the calculation of the feasible set ${\cal R}_p(\hat\mu(
 \al(\cdot), w_0, \omega), \tilde{\bmu})$ that contrasts an alpha investing
 testimator with the risk-inflation estimator $\tilde{\bmu}$.  The
 risk-inflation estimator has no wealth constraint; calculations need only track
 the wealth of the alpha investing estimator, which we abbreviate as
 $\hat{\bmu}$ with the understanding that it depends on the choice of the
 investing function $\al(\cdot)$, $w_0$, and $\omega$ throughout this section.
  Let
 \begin{equation}
   {\cal U}^\theta(\hat{\bmu},\tilde{\bmu}) = 
       \max \evsub{\bmu} \Bigl(
       \cos(\theta) R(\hat{\bmu}, \bmu) + \sin(\theta) R(\tilde{\bmu},\bmu) 
       \Bigr)
 \label{eq:U}
 \end{equation}
 denote the maximum expected value with respect to a stochastic process $\bmu$
 of the weighted sum of risks defined by the angle $ 0 \le \theta \le 2 \pi$.
  Let $\bmu^\theta$ denote the mean process that maximizes $U^\theta$.  The
 point $\evsub{\bmu^\theta}(R(\hat{\bmu}, \bmu), R(\tilde{\bmu}, \bmu))$ lies on
 the boundary of ${\cal R}_p(\hat{\bmu},\tilde{\bmu})$ where the feasible risk
 set is tangent to the line defined by the mixture weights in \eqn{eq:U}.  Plots
 that show the feasible risk set, such as Figure \ref{fig:riFeasibleSet},
 highlight the boundary points that are explicitly computed.  By varying
 $\theta$ over the circle, we approximate the feasible risk set as the
 intersection of the resulting half-planes.

 
 We compute ${\cal U}^\theta$ via numerical backward induction.  This induction
 approximates the wealth of the alpha investing testimator on a grid.  The
 wealth grid $G$ spans the minimal attainable wealth ($w_0 - \sum_{j=1}^p
 \al_j$) to a maximum allowed wealth, which we set to $w_{\max} = 5$.  (Our
 results have not been sensitive to the choice of $w_{\max}$ so long as it is
 substantially larger than $w_0 + \omega$.)  The wealth grid is
 `logarithmically' spaced at $N$ points, with a finer spacing 0.0001 for small
 wealths below 0.001 and gradually larger spacing as the wealth increases.  We
 insure that the grid includes an element $G_{k_0} = w_0$.


 The $p \times N$ matrix $U^{\theta}$ holds intermediate calculations of the
 expected value ${\cal U}^{\theta}$.  The rows in this matrix identify the hypothesis $H_j$
 and the columns index the position in the wealth grid $G$.  We fill $U^\theta$
 from the `bottom up' in a tail recursion.  At the completion of the
 calculations,
 \begin{eqnarray}
   U^\theta_{jk} &=&  \max_\mu \;\Bigl\{
     \cos(\theta) R(\hat\mu_{\al(G_k)}, \mu) + \sin(\theta) R(\tilde{\mu},\mu) \cr
     & &+ r_\mu\left(\al(G_k)\right) \;
          \left(c \,U^{\theta}_{j+1,k_c+1} + (1-c) U^{\theta}_{j+1,k_c}\right) \cr
     & & + \left(1- r_\mu(\al(G_k))\right) \; 
          \left(d \,U^{\theta}_{j+1,k_d+1} + (1-d) U^{\theta}_{j+1,k_d}\right)   
     \Bigr\}
 \label{eq:Ujk}
 \end{eqnarray}
 for $j=p, p-1,\ldots,1$ and $k = 1,\ldots,N$ and the boundary condition
 $U^\theta_{p+1,k} = 0$.  Note the similarity to expression \eqn{eq:RMuHat}.  At
 the completion of the computation, ${\cal U}^\theta = U^{\theta}_{1,k_0}$.  The
 first line of \eqn{eq:Ujk} adds the contribution to the weighted risk from
 testing $H_j$ at the alpha level $\al(G_k)$.  The second line adds the expected
 subsequent risk if the test rejects $H_j$, which occurs with probability
 $r_\mu(\al(G_k))$.  If the test rejects, the alpha wealth increases to $G_k -
 \al(G_k) + \omega$.  This wealth is unlikely to match that at any grid
 position, so we linearly interpolate between positions $k_c$ and $k_c+1$ so
 that $G_{k_c} \le G_k - \al(G_k) + \omega \le G_{k_c+1}$ and set the weight $c$
 in \eqn{eq:Ujk} to $c = (G_k - \al(G_k) + \omega)/(G_{k_c+1}-G_{k_c})$.
  Similarly, the third line of \eqn{eq:Ujk} adds the expected contribution if
 the testimator does not reject $H_j$ and its wealth falls to $G_k - \al(G_k)$.

 \remark{ One need not store the full matrix $U^\theta$, but its use simplifies
 the description of the algorithm.  One only needs the next row $U^\theta_{j+1,
 \cdot}$ to compute $U_{j,\cdot}$.  Such space saving -- using just two rows
 rather than the full matrix -- becomes essential in problems that track a
 larger state space.  Note also that one can cache the indices and weights
 ($k_c, c$ and $k_d, d$) prior to the recursion because these can be determined
 from the grid positions and $\omega$ and remain fixed throughout the backward
 recursion.}


 The feasible set that compares the testimators defined by two alpha investing
 rules $\al(\cdot)$ and $\beta(\cdot)$ requires a more complex recursion that
 must track the wealths of both.  The linear grid $G$ remains, but the matrix
 $U^\theta$ defined in \eqn{eq:Ujk} becomes a three dimensional tensor of size
 $p \times N \times N$.  The calculation is essentially a more messy version of
 \eqn{eq:Ujk} but for one nuance that we want to emphasize.  To simplify the
 presentation, we suppress the linear interpolation and pretend that all of the
 concerned wealths are represented in the wealth grid.  If the alpha investing
 rule $\al(\cdot)$ with wealth $G_k$ rejects $H_j$, its wealth goes from $G_k$
 to $G_{k+}$; if it fails to reject, its wealth falls to $G_{k-}$.  Similarly,
 we use $\ell+$ and $\ell-$ for the positions for the rule defined by
 $\beta(\cdot)$.  If we assume $\al(G_k) < \beta(G_\ell)$, then the recursion
 can be written as
 \begin{eqnarray}
   U^\theta_{jk\ell} &=&  \max_\mu \;\Bigl\{
     \cos(\theta) R(\hat\mu_{\al(G_k)}, \mu) 
       + \sin(\theta) R(\hat\mu_{\beta(G_\ell)},\mu) \cr
     & & + r_\mu\left(\al(G_k)\right) \; U_{j+1,k+,\ell+} 
         + \left[ r_\mu(\beta(G_\ell)) - r_\mu(\al(G_k)) \right] \, U_{j+1,k-,\ell+} \cr
     & & + \left[1-r_\mu(\beta(G_\ell))\right] \, U_{j+1,k-,\ell-} \, \Bigr\}\;,
 \label{eq:Ujkl}
 \end{eqnarray}
 where the boundary condition is $U_{p+1,\cdot,\cdot}^\theta= 0$.  The first
 line in \eqn{eq:Ujkl} is the expected risk produced by the test of $H_j$, and
 following summands denote the expected contributions to the risk if both
 reject, if only the rule with the larger alpha level rejects, and if neither
 rejects.  The point of writing this out is to emphasize these testimators see
 the same data, not independent samples.  Hence, $\al(G_k) < \beta(G_\ell)$
 implies that if the first rule $\al(\cdot)$ rejects $H_j$, then the second rule
 must also reject $H_j$ because it tests the same hypothesis using the same
 data, but with a larger alpha level.


%--------------------------------------------------------------------------
\section{ Discussion }
%--------------------------------------------------------------------------

 Feasible risk sets allow us to study the risks of testimators in sequential
 problems.  The comparisons shown here suggest that universal alpha investing
 does well and can compete with the risks attained by any geometric procedure.
  It is also of interest to point out that a large initial alpha wealth and
 payout $w_0 = \omega = 0.25$ produce a noticeable reduction in the risk (Figure
 \ref{fig:univRI}).  This choice for $\omega$ implies that controlling the
 expected false discovery rate at 25\%, quite a bit larger than would usually be
 chosen, produces lower risk unless the mean process is quite sparse.


 A particular benefit of these computations is that they suggest conjectures
 about asymptotic properties of these estimators.  For example, it appears that
 we can approximate the boundary of the feasible risk set using two-point models
 defined in \eqn{eq:muj}.  Figure \ref{fig:riFeasibleSet} shows that by varying
 the signal probability $\pi$ such a model can be found that approaches the
 boundary of the feasible risk set.  Further, the simulation shown in Figure
 \ref{fig:simRisk} shows that (at least for this location) the boundary mean
 process generates either zero or approximately a single, non-zero value.
  Hence, it would appear that, asymptotically in $p$, there exists a two-point
 model $(\pi,\,\mu^{*})$ for which the risks lie within some epsilon ball of the
 boundary of the feasible risk set.


 The shapes of the various feasible sets are also intriguing.  For instance in
 Figure \ref{fig:riFeasibleSet}, the set has a vertical segment where the risk
 of the oracle attains its maximum at $p$.  These risks occur when the mean
 process is saturated in the sense that every $\mu_j^2 \ge 1$ so that the
 risk-inflation oracle ``fits everything.''.  Although the oracle then has fixed
 risk $p$, the risk of the testimator varies with the size of $\mu_j$.  This
 property of the feasible risk set for saturated mean processes is rather
 different from the behavior for sparse processes.  As the risk of the oracle
 estimator approaches its minimum, the feasible risk set approaches a single
 point.  That the set comes to a point is not surprising.  Unlike the saturated
 case, a unique process produces the minimum risk, namely the process for which
 every $\mu_j = 0$.  What is surprising is the lack of evident curvature.  Does
 the feasible set come to a point or instead form a very tight curve?
 

 As a final conjecture, the performance of testimators derived from the
 universal rule $\al_u(\cdot)$ suggests that this investing rule can simplify
 the choice of an alpha investing method.  Figure \ref{fig:univGeo} shows its
 ability to match the risks obtained by various geometric testimators.  Ideally,
 we would like to obtain results analogous to those in \citet{rissanen83} that
 show that alpha investing is about as good as one can do.  Such a proof would
 then simplify the use of alpha investing in practice as one would not need to
 struggle with the choice of an investing rule but instead could focus on
 generating a better stream of features.


%--------------------------------------------------------------------------
\section*{Acknowledgement}
%--------------------------------------------------------------------------

The authors thank NSF.


%--------------------------------------------------------------------------
% References
%--------------------------------------------------------------------------

\bibliography{../../../../references/stat}
\bibliographystyle{../../bst/asa}

\end{document} %==========================================================

% LocalWords:  jk
